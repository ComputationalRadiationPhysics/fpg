\section{Messungen auf NVIDIA-GPUs}
\label{nvidia}

\subsection{Verwendete Hard- und Software}

Die hier gezeigten Benchmark-Ergebnisse wurden auf einem Knoten der
\texttt{gpu2}-Partition des \gls{hpc}-Systems Taurus gemessen, der über zwei
GPUs des Modells Tesla K20x (mit aktiviertem ECC) verfügt.

Die Messungen fanden innerhalb der SCS5-Umgebung statt, in der die folgende
Software verwendet wurde:

\begin{itemize}
    \item \texttt{CUDA/10.0.130} (Modulsystem)
    \item \texttt{GCC/7.3.0-2.30} (Modulsystem)
    \item \gls{hip} (Version 1.5.19061, ROCm-GitHub-Repository)
    \item ComputeCpp (Codeplays SYCL-Implementierung, Version 1.0.5 für Ubuntu
          14.04)
\end{itemize}

\subsection{zcopy}

\subsubsection{Vorüberlegungen}
\label{nvidia:zcopy:vorueberlegungen}

Um eine optimale Ausnutzung der Speicherbandbreite zu erreichen, ist ein genauer
Blick auf die verwendete Architektur erforderlich. Die K20x-GPUs weisen hier
größere Unterschiede zu den P100- und V100-Modellen auf, die sich auf den
betrachteten Benchmark auswirken können. Während die P100/V100-GPUs auf jedem
Multiprozessor bei einfacher Genauigkeit \num{64} Hardware-Threads zur Verfügung
haben, sind es bei den K20x-GPUs \num{192}. Ein Multiprozessor der
P100/V100-GPUs kann somit zwei \textit{Warps} parallel ausführen, während auf
der K20x-GPU sechs parallele \textit{Warps} pro Multiprozessor möglich sind. Die
unterschiedliche \textit{Warp}-Zahl wirkt sich unter Umständen auf die
Bandbreite des Speichers aus, da jeder K20x-Multiprozessor mehr Zugriffe
benötigt als seine P100/V100-Gegenstücke. Aus diesem Grund werden auf der
K20x-GPU und den P100/V100-GPUs Blockgrößen untersucht, die ein Vielfaches von
\num{64} sind (bis \num{1024}). Zusätzlich werden auf der K20x Blockgrößen
betrachtet, die ein Vielfaches von \num{192} sind (bis \num{768}), um eventuelle
Effekte zu entdecken.

Alle verwendeten NVIDIA-GPUs besitzen eine L1-Cacheline-Größe von \num{128}
Byte. Speicherzugriffe der \textit{Warps} werden auf \textit{Half-Warps} (also
\num{16} Threads des \textit{Warps}) aufgeteilt. Sofern alle Threads eines
\textit{Half Warps} benachbarte \num{8}-Byte-Sequenzen laden, lassen sich diese
Zugriffe in einer einzigen Cacheline vereinen, was die pro Thread verwendete
Speicherbandbreite drastisch reduziert und eine insgesamt höhere Auslastung
erlaubt.

Um eine höhere Auslastung des Speicher-Controllers zu erreichen und die Anzahl
der notwendigen Schleifendurchläufe innerhalb des Kernels zu verringern, wodurch
sich die Zahl abhängiger Instruktionen verringert, werden die \num{8} Byte pro
Thread in diesem Benchmark verdoppelt. Jeder Thread lädt also \num{16} Byte in
Form eines Elements vom Typ \texttt{float4}. Die Anhang befindlichen
Quelltexte~\ref{anhang:cuda:zcopy} (CUDA), \ref{anhang:hip:zcopy} (\gls{hip})
und \ref{anhang:sycl:zcopy} (SYCL) zeigen die Kernel-Implementierungen der
verschiedenen Spracherweiterungen.

\subsubsection{Messmethoden}
\label{nvidia:zcopy:methoden}

Der Speicher der GPU wurde zunächst mit zwei gleich großen Datenfeldern $A$ und
$B$ befüllt, die jeweils etwas weniger als die Hälfte des GPU-Speichers
einnahmen und $n$ \texttt{float4}-Elemente umfassten. Diese wurden mit Nullen
($A$) bzw.\ \textit{NaN} ($B$) initialisiert. Anschließend wurden die Kernel für
jede Block-Größe jeweils zehn Mal ausgeführt, wobei für jeden Kernel-Durchlauf
die benötigte Zeit über die Event-Funktionalitäten der verschiedenen
Spracherweiterungen gemessen wurde. Der minimale Zeitbedarf $t_{\text{min}}$
(in \si{\second}) jedes Kernels wird als Grundlage für die Berechnung der
Bandbreite $B$ (in \si{\gibi\byte\per\second}) genommen:

\[
    B = \frac{k \cdot \text{sizeof(\texttt{float4})} \cdot n}{t_\text{min}},
\]

wobei der Faktor $k$ die Werte $2 \cdot 10^{-9}$ (kombinierter Lese- und
Schreibzugriff) oder $1 \cdot 10^{-9}$ (Schreibzugriff) annehmen kann.

Der Quelltext~\ref{nvidia:zcopy:befehle} zeigt die verwendeten Compiler-Flags.

\begin{code}
    \begin{minted}[fontsize=\small]{bash}
# CUDA-Compiler
nvcc -std=c++14 -O3 -gencode arch=compute_35,code=sm_35

# HIP-Compiler
hipcc -O3 -std=c++14 -gencode arch=compute_35,code=sm_35

# SYCL-Compiler
compute++ -std=c++17 -O3 -sycl-driver -sycl-target ptx64
    \end{minted}
    \caption{Compiler-Flags für zcopy}
    \label{nvidia:zcopy:befehle}
\end{code}

\subsubsection{Ergebnisse}

Ein Blick auf die mit CUDA ermittelten Ergebnisse zeigt, dass die K20x-GPU bei
Speicherzugriffen von größeren Thread-Blöcken als auch von vielen Thread-Blöcken
profitiert. Dies gilt sowohl für die \glqq generischen\grqq\ Blockgrößen, die
ein Vielfaches von \num{64} darstellen (siehe die
Abbildungen~\ref{nvidia:zcopy:k20xgenerischrw} und
\ref{nvidia:zcopy:k20xgenerischw}), als auch die auf die Kepler-Architektur
angepassten Blockgrößen (siehe die
Abbildungen~\ref{nvidia:zcopy:k20xangepasstrw} und
\ref{nvidia:zcopy:k20xangepasstw}). Außerdem bestätigt sich die Annahme, dass
mit der angepassten Blockgröße eine etwas bessere Bandbreite erzielt werden kann
(siehe Abbildung~\ref{nvidia:zcopy:k20xvergleich}). Dies hilft insbesondere dem
kombinierten Lese- und Schreibvorgang, der dadurch bei einer großen Blockzahl
nahezu die selbe Bandbreite wie der reine Schreibvorgang erreicht (siehe
Abbildung~\ref{nvidia:zcopy:k20xrwwo}).

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- Lesen + Schreiben -- generisch -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 40, ymax = 190,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-64,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 64$} 

            \addplot table [x = blocks_per_sm, y = throughput-128,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 128$} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 256$} 

            \addplot table [x = blocks_per_sm, y = throughput-512,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 512$} 

            \addplot table [x = blocks_per_sm, y = throughput-1024,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 1024$} 
        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für generische Blöcke (Lesen und Schreiben, CUDA)}
    \label{nvidia:zcopy:k20xgenerischrw}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- Schreiben -- generisch -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 70, ymax = 190,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-64,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-w.csv};
            \addlegendentry{$\text{Blockgröße} = 64$} 

            \addplot table [x = blocks_per_sm, y = throughput-128,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-w.csv};
            \addlegendentry{$\text{Blockgröße} = 128$} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-w.csv};
            \addlegendentry{$\text{Blockgröße} = 256$} 

            \addplot table [x = blocks_per_sm, y = throughput-512,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-w.csv};
            \addlegendentry{$\text{Blockgröße} = 512$} 

            \addplot table [x = blocks_per_sm, y = throughput-1024,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-w.csv};
            \addlegendentry{$\text{Blockgröße} = 1024$} 
        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für generische Blöcke (Schreiben, CUDA)}
    \label{nvidia:zcopy:k20xgenerischw}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- Lesen + Schreiben -- optimiert -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 40, ymax = 190,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-192,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 192$} 

            \addplot table [x = blocks_per_sm, y = throughput-384,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 384$} 

            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-rw.csv};
            \addlegendentry{$\text{Blockgröße} = 768$} 

        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für optimierte Blöcke (Lesen und Schreiben, CUDA)}
    \label{nvidia:zcopy:k20xangepasstrw}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- Schreiben -- optimiert -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 70, ymax = 190,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-192,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{$\text{Blockgröße} = 192$} 

            \addplot table [x = blocks_per_sm, y = throughput-384,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{$\text{Blockgröße} = 384$} 

            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{$\text{Blockgröße} = 768$} 
        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für optimierte Blöcke (Schreiben, CUDA)}
    \label{nvidia:zcopy:k20xangepasstw}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- Schreiben -- generisch und optimiert -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 175, ymax = 188,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{$\text{Blockgröße} = 768$} 

            \addplot table [x = blocks_per_sm, y = throughput-1024,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-generic-w.csv};
            \addlegendentry{$\text{Blockgröße} = 1024$} 
        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für generische und optimierte Blöcke (Schreiben,
             CUDA)}
    \label{nvidia:zcopy:k20xvergleich}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- optimiert -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            % ymin = 175, ymax = 188,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-rw.csv};
            \addlegendentry{Lesen + Schreiben} 

            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{Schreiben} 
        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für 768er-Blöcke (CUDA)}
    \label{nvidia:zcopy:k20xrwwo}
\end{figure}

Die mit \gls{hip} und SYCL ermittelten Ergebnisse zeigen ein ähnliches Bild
(siehe die angehängten Abbildungen in den
Abschnitten~\ref{anhang:hip:nvzcopyfig} (\gls{hip}) und
\ref{anhang:sycl:zcopyfig} (SYCL)).

Hinsichtlich der erreichten Speicherbandbreite bestehen zwischen den
Spracherweiterungen keine nennenswerten Unterschiede, wie die
Abbildung~\ref{nvidia:zcopy:k20xexts} zeigt.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- Schreiben -- optimiert -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 175, ymax = 190,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{CUDA} 

            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-hip-k20x-optimized-w.csv};
            \addlegendentry{HIP} 

            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-sycl-k20x-optimized-w.csv};
            \addlegendentry{SYCL} 
        \end{axis}
    \end{tikzpicture}
    \caption{K20x-Bandbreite für 768er-Blöcke (Schreiben)}
    \label{nvidia:zcopy:k20xexts}
\end{figure}

Die K20x-GPU erreicht etwa 75\% der theoretisch möglichen Bandbreite (siehe
Abbildung~\ref{nvidia:zcopy:k20xmax}). Dies ist durch das aktivierte ECC und
die auf dem Taurus-System nicht steuerbaren Taktraten der GPU zu erklären. Für
den Reduction-Benchmark wird daher eine tatsächlich erreichbare Obergrenze von
\SI{188}{\gibi\byte\per\second} angenommen.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {zcopy -- optimiert -- K20x},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 8192,
            ymin = 70, ymax = 260,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            extra y ticks = 249.6,
            extra y tick labels = {$249.6$},
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-rw.csv};
            \addlegendentry{Lesen+Schreiben} 

            \addplot table [x = blocks_per_sm, y = throughput-768,
                            col sep = semicolon]
                           {data/zcopy-nvidia-cuda-k20x-optimized-w.csv};
            \addlegendentry{Schreiben} 

            \addplot table [x = blocks_per_sm, y = peak, col sep = semicolon]
                           {data/zcopy-nvidia-k20x-peak.csv};
            \addlegendentry{Peak}
        \end{axis}
    \end{tikzpicture}
    \caption{Theoretische und praktische K20x-Bandbreite für 768er-Blöcke}
    \label{nvidia:zcopy:k20xmax}
\end{figure}

\subsection{Reduction}

\subsubsection{Implementierung}

Die Implementierungen des Reduktionskernels sind dieser Arbeit angehängt und
befinden sich in den Quelltexten~\ref{anhang:cuda:reduction} (CUDA),
\ref{anhang:hip:reduction} (\gls{hip}) und \ref{anhang:sycl:reduction} (SYCL).

\subsubsection{Messmethoden}
\label{nvidia:reduction:methoden}

Zunächst wurde die zu reduzierende Array-Größe auf die größte in den
GPU-Speicher passende Elementezahl festgesetzt. Durch das Ausprobieren mehrerer
Block-Größen zwischen \num{64} und \num{1024} einerseits und der Variation der
Block-Zahl pro Multiprozessor andererseits wurde dann eine optimierte
Kernel-Konfiguration ermittelt.

Im nächsten Schritt wurde die ermittelte optimierte Konfiguration auf Arrays
verschiedener Größe ausprobiert, um das Skalierungsverhalten festzustellen.

Jede für den beschriebenen Ablauf nötige Kernel-Ausführung erfolgte zehn Mal
hintereinander. Für jede Ausführung wurde über die den Spracherweiterungen
zugehörigen Events die Laufzeit ermittelt; die minimale Laufzeit diente als
Grundlage für die Berechnung der Bandbreite.

Die Compiler-Flags sind dieselben wie die für den zcopy-Benchmark in
Abschnitt~\ref{nvidia:zcopy:methoden} aufgeführten.

\subsubsection{Ergebnisse}

Der Blick auf die Abbildung~\ref{nvidia:reduction:cuda} zeigt, dass \num{256},
\num{512} und \num{1024} Threads die besten Block-Größen bilden. Die selben
Ergebnisse zeigen sich für \gls{hip} (siehe
Abbildung~\ref{anhang:nvidia:reduction:hip}) und SYCL (siehe
Abbildung~\ref{anhang:nvidia:reduction:sycl}). Die SYCL-Variante zeigt zudem ein
effizienteres Verhalten für Blöcke mit \num{128} Threads.  Da \num{256} Threads
über alle Spracherweiterungen hinweg die  größte Auswahl an guten
Block-pro-Multiprozessor-Konfigurationen bieten, wurde im weiteren
Benchmark-Verlauf diese Größe ausgewählt. Zwischen \num{8} und \num{1024}
Blöcken pro Multiprozessor kommen für diese Größe verschiedene Konfigurationen
in Frage, die sich für große Arrays nahezu gleichwertig verhalten. Da die
nächste Benchmark-Stufe auch weitaus kleinere Arrays umfasst, wurde hier die
kleinste sinnvolle Konfiguration gewählt: \num{8} Blöcke pro Multiprozessor. 

Die zcopy-Ergebnisse können hier bestätigt werden: zwischen den Erweiterungen
bestehen allenfalls marginale Bandbreitenunterschiede. Zudem kann die
tatsächlich erreichbare Bandbreite, die im zcopy-Benchmark ermittelt wurde,
auch bei der Reduktion nahezu erreicht werden (siehe
Abbildung~\ref{nvidia:reduction:peak}).

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Reduction -- K20x -- CUDA},
            xlabel = {Blöcke pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 2, xmax = 4096,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks, y = dim64,
                            col sep = semicolon]
                           {data/reduce-nvidia-cuda-k20x.csv};
            \addlegendentry{Blockgröße = 64} 

            \addplot table [x = blocks, y = dim128,
                            col sep = semicolon]
                           {data/reduce-nvidia-cuda-k20x.csv};
            \addlegendentry{Blockgröße = 128} 

            \addplot table [x = blocks, y = dim256,
                            col sep = semicolon]
                           {data/reduce-nvidia-cuda-k20x.csv};
            \addlegendentry{Blockgröße = 256} 

            \addplot table [x = blocks, y = dim512,
                            col sep = semicolon]
                           {data/reduce-nvidia-cuda-k20x.csv};
            \addlegendentry{Blockgröße = 512} 

            \addplot table [x = blocks, y = dim1024,
                            col sep = semicolon]
                           {data/reduce-nvidia-cuda-k20x.csv};
            \addlegendentry{Blockgröße = 1024} 
        \end{axis}
    \end{tikzpicture}
    \caption{Reduction-Bandbreite K20x (CUDA)}
    \label{nvidia:reduction:cuda}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Reduction -- K20x},
            xlabel = {$n$},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 65536, xmax = 134217728,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = n, y = CUDA,
                            col sep = semicolon]
                           {data/reduce-nvidia-k20x.csv};
            \addlegendentry{HC} 

            \addplot table [x = n, y = HIP,
                            col sep = semicolon]
                           {data/reduce-nvidia-k20x.csv};
            \addlegendentry{HIP} 

            \addplot table [x = n, y = SYCL,
                            col sep = semicolon]
                           {data/reduce-nvidia-k20x.csv};
            \addlegendentry{SYCL} 

            \addplot table [x = , y = Peak, col sep = semicolon]
                           {data/reduce-nvidia-k20x.csv};
            \addlegendentry{Peak}
        \end{axis}
    \end{tikzpicture}
    \caption{Reduction-Bandbreite der K20x (acht 256er-Blöcke pro
             Multiprozessor)}
    \label{nvidia:reduction:peak}
\end{figure}

\subsection{N-Body}

\subsubsection{Implementierung}

Die theoretische Funktion~\ref{methoden:nbody:gpu:bodybodyinteraction}, die die
Interaktion zwischen zwei Körpern berechnet, wurde in allen Spracherweiterungen
umgesetzt. Durch den Einsatz von \gls{fma}-Operationen werden die benötigten
\gls{flops} für die Berechnung des Skalarprodukts sowie der Beschleunigung
verringert. Die inverse Wurzel wird durch die \texttt{rsqrt}-Funktion berechnet.
Die Quelltexte~\ref{anhang:cuda:bodybodyinteraction} (CUDA),
\ref{anhang:hip:bodybodyinteraction} und \ref{anhang:sycl:bodybodyinteraction}
(SYCL) im Anhang dieser Arbeit zeigen die konkrete Implementierung.

Die theoretischen Funktionen~\ref{methoden:nbody:gpu:tilecalculation} und
\ref{methoden:nbody:gpu:calcforces} wurden zusammengefasst, da erstere
nur aus einer kurzen Schleife besteht. Überdies wurde der Compiler angewiesen,
die Schleife auszurollen (siehe auch den nächsten Abschnitt). Diese
Implementierungen finden sich in den angehängten
Quelltexten~\ref{anhang:cuda:forcecalculation} (CUDA),
\ref{anhang:hip:forcecalculation} (HIP) bzw. \ref{anhang:sycl:forcecalculation}
(SYCL).

Ein gesonderter Vergleich ist zwischen CUDA und SYCL nötig, da das
experimentelle Backend für NVIDIA-GPUs der ComputeCpp-Implementierung die
Funktion \texttt{rsqrtf} für die reziproke Quadratwurzel nicht unterstützt. Da
die äquivalente Berechnung \texttt{1.f / sqrtf} deutlich langsamer ist, wurde
stattdessen eine schnellere, weniger genaue Implementierung der Funktion
\texttt{rsqrtf} aus dem Quelltext des Spiels \textit{Quake 3 Arena} übernommen
(siehe Quelltext~\ref{nvidia:nbody:qrsqrt} für eine normale
C++-Implementierung).

\begin{code}
    \begin{minted}[fontsize=\small]{c++}
auto Q_rsqrt(float number) -> float
{
    auto x2 = number * 0.5f;
    auto y = number;
    auto i = *(reinterpret_cast<std::int32_t*>(&y));

    i = 0x5f3759df - (i >> 1);

    y = *(reinterpret_cast<float*>(&i));
    y *= 1.5f - (x2 * y * y);

    return y;
}
    \end{minted}
    \caption{Quake-3-Implementierung der rsqrt-Funktion}
    \label{nvidia:nbody:qrsqrt}
\end{code}

\subsubsection{Optimierung und Auswertung}
\label{nvidia:nbody:auswertung}

Eine einfache Optimierung ist das Ausrollen der Schleife, die nacheinander die
Interaktionen berechnet. Dadurch erhöht sich der Registerbedarf pro Thread, der
Overhead, der durch Verzweigungsinstruktionen anfällt, wird jedoch verringert.
Dieser Effekt ist deutlich in der Abbildungen~\ref{nvidia:nbody:unroll:cuda} zu
sehen:  durch die Bestimmung eines besseren Ausrollfaktors lassen sich in diesem
Benchmark bei einer festen Kachelgröße von $p = 256$ knapp \num{600} GFLOPS mehr
Durchsatz gewinnen. Dieses Verhalten kann auch bei der Implementierung mit
\gls{hip} (siehe Abbildung~\ref{anhang:nvidia:nbody:unroll:hip}) beobachtet
werden, nicht jedoch bei SYCL (siehe
Abbildung~\ref{nvidia:nbody:unroll:sycl}). Hier ist zu vermuten, dass der
Compiler den Ausrollfaktor ignoriert oder nicht zu verarbeiten weiß.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Ausrollen -- CUDA},
            xlabel = {Ausrollfaktor},
            ylabel = {GFLOPS},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 1024,
            xtick = {1,2,4,8,16,32,64,128,256,512,1024},
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = count, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-unroll-524288.csv};
            \addlegendentry{$n = 524.288$} 

            \addplot table [x = count, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-unroll-65536.csv};
            \addlegendentry{$n = 65.536$} 

            \addplot table [x = count, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-unroll-8192.csv};
            \addlegendentry{$n = 8.192$} 
        \end{axis}
    \end{tikzpicture}
    \caption{Performanzgewinn durch das Ausrollen der Schleife (CUDA)}
    \label{nvidia:nbody:unroll:cuda}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Ausrollen -- SYCL},
            xlabel = {Ausrollfaktor},
            ylabel = {GFLOPS},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 1024,
            xtick = {1,2,4,8,16,32,64,128,256,512,1024},
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = count, y = SYCL, col sep = semicolon]
                           {data/nbody-nvidia-unroll-524288.csv};
            \addlegendentry{$n = 524.288$} 

            \addplot table [x = count, y = SYCL, col sep = semicolon]
                           {data/nbody-nvidia-unroll-65536.csv};
            \addlegendentry{$n = 65.536$} 

            \addplot table [x = count, y = SYCL, col sep = semicolon]
                           {data/nbody-nvidia-unroll-8192.csv};
            \addlegendentry{$n = 8.192$} 
        \end{axis}
    \end{tikzpicture}
    \caption{Performanzgewinn durch das Ausrollen der Schleife (SYCL)}
    \label{nvidia:nbody:unroll:sycl}
\end{figure}

Es ist außerdem festzustellen, dass der Ausrollfaktor auf \gls{hip} einen
ähnlichen Einfluss wie auf CUDA hat. \gls{hip} erreicht jedoch bei kleinen und
großen Faktoren weniger \gls{flops} als CUDA, während mittlere Faktoren ähnliche
Ergebnisse erzielen (siehe Abbildung~\ref{nvidia:nbody:unroll:cudahip}).
Möglicherweise verhindern das Ummanteln des CUDA-\gls{api} durch das
\gls{hip}-\gls{api} oder der Aufruf des \texttt{nvcc}-Compilers durch das
\texttt{hipcc}-Skript einige kleinere Compiler-Optimierungen, die bei nativem
CUDA-Code möglich wären.

Anhand dieser Messung wurde für den weiteren Verlauf ein Ausrollfaktor von 64
festgelegt.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Ausrollen -- CUDA und HIP},
            xlabel = {Ausrollfaktor},
            ylabel = {GFLOPS},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 1024,
            xtick = {1,2,4,8,16,32,64,128,256,512,1024},
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = count, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-unroll-524288.csv};
            \addlegendentry{CUDA} 

            \addplot table [x = count, y = HIP, col sep = semicolon]
                           {data/nbody-nvidia-unroll-524288.csv};
            \addlegendentry{HIP} 
        \end{axis}
    \end{tikzpicture}
    \caption{Vergleich der Ausrollfaktoren zwischen CUDA und HIP ($n = 524.288$)}
    \label{nvidia:nbody:unroll:cudahip}
\end{figure}

Der nächste performanzrelevante Faktor ist die Größe der Kacheln selbst. Aus den
in der Abbildung~\ref{nvidia:nbody:tilesize:cuda} dargestellten Messergebnissen
wird ersichtlich, dass die gewählte Kachelgröße auf die Leistung einen
erheblichen Einfluss haben kann. Auch dieses Verhalten ist bei \gls{hip} (siehe
Abbildung~\ref{anhang:nvidia:nbody:tilesize:hip}) und SYCL (siehe
Abbildung~\ref{anhang:nvidia:nbody:tilesize:sycl}) feststellbar. Für den
weiteren Messverlauf wird daher eine Kachelgröße von $p = 512$ angenommen.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Kachelgrößen -- CUDA},
            xlabel = {Kachelgröße},
            ylabel = {GFLOPS},
            xmode = log,
            log basis x = 2,
            xtick = {32,64,128,256,512,1024},
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = tilesize, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-tilesize-524288.csv};
            \addlegendentry{$n = 524.288$} 

            \addplot table [x = tilesize, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-tilesize-65536.csv};
            \addlegendentry{$n = 65.536$} 

            \addplot table [x = tilesize, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia-tilesize-8192.csv};
            \addlegendentry{$n = 8.192$} 
        \end{axis}
    \end{tikzpicture}
    \caption{Performanz bei verschiedenen Kachelgrößen (CUDA)}
    \label{nvidia:nbody:tilesize:cuda}
\end{figure}

Mit der experimentell ermittelten Konfiguration lässt sich ein direkter
Vergleich zwischen CUDA und \gls{hip} einerseits sowie CUDA (mit
\texttt{Q\_rsqrt}) und SYCL andererseits anstellen. Die
Abbildung~\ref{nvidia:nbody:comparisonhip} zeigt, dass die Performanz bei CUDA
und \gls{hip} nahezu identisch ist. Der Blick in den generierten Maschinen-Code
zeigt, dass der \texttt{nvcc}-Compiler und der \texttt{hcc}-Wrapper um
\texttt{nvcc} dasselbe Ergebnis erzeugen (siehe
Quelltexte~\ref{nvidia:nbody:ptxnvcc} und \ref{nvidia:nbody:ptxhip}).

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Leistungsvergleich -- CUDA und HIP},
            xlabel = {$n$},
            ylabel = {GFLOPS},
            xtick = data,
            xmode = log,
            log basis x = 2,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma,
            ybar,
            width = 0.75\textwidth,
            scale only axis,
            ymin = 0, ymax = 4000,
            extra y ticks = 3935,
            extra y tick labels = {\num{3935}},
            extra y tick style={grid=major, major grid style={solid,thick,draw=red}},
            scaled y ticks = false,
            ylabel near ticks,
            xlabel near ticks
        ]
            \addplot table [x = n, y = CUDA, col sep = semicolon]
                           {data/nbody-nvidia.csv};
            \addlegendentry{CUDA} 

            \addplot table [x = n, y = HIP, col sep = semicolon]
                           {data/nbody-nvidia.csv};
            \addlegendentry{HIP} 
        \end{axis}
    \end{tikzpicture}
    \caption{Leistungsvergleich zwischen CUDA und HIP}
    \label{nvidia:nbody:comparisonhip}
\end{figure}

\begin{figure}
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{minted}[fontsize=\footnotesize]{text}
shl.b32 %r29, %r42, 4;
add.s32 %r31, %r22, %r29;
ld.shared.v4.f32 {%f50, %f51, %f52,
                  %f53}, [%r31];
sub.f32 %f58, %f50, %f35;
sub.f32 %f59, %f51, %f36;
sub.f32 %f60, %f52, %f37;
mov.f32 %f61, 0f358637BE;
fma.rn.f32 %f62, %f60, %f60, %f61;
fma.rn.f32 %f63, %f59, %f59, %f62;
fma.rn.f32 %f64, %f58, %f58, %f63;
mul.f32 %f65, %f64, %f64;
mul.f32 %f66, %f64, %f65;
rsqrt.approx.f32 %f67, %f66;
mul.f32 %f68, %f53, %f67;
fma.rn.f32 %f69, %f58, %f68, %f21590;
fma.rn.f32 %f70, %f59, %f68, %f21589;
fma.rn.f32 %f71, %f60, %f68, %f21588;
        \end{minted}
        \captionof{listing}{Maschinencode des CUDA-Kernels}
        \label{nvidia:nbody:ptxnvcc}
    \end{minipage}
    %
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{minted}[fontsize=\footnotesize]{text}
shl.b32 %r29, %r42, 4;
add.s32 %r31, %r22, %r29;
ld.shared.v4.f32 {%f50, %f51, %f52,
                  %f53}, [%r31];
sub.f32 %f58, %f50, %f35;
sub.f32 %f59, %f51, %f36;
sub.f32 %f60, %f52, %f37;
mov.f32 %f61, 0f358637BE;
fma.rn.f32 %f62, %f60, %f60, %f61;
fma.rn.f32 %f63, %f59, %f59, %f62;
fma.rn.f32 %f64, %f58, %f58, %f63;
mul.f32 %f65, %f64, %f64;
mul.f32 %f66, %f64, %f65;
rsqrt.approx.f32 %f67, %f66;
mul.f32 %f68, %f53, %f67;
fma.rn.f32 %f69, %f58, %f68, %f21590;
fma.rn.f32 %f70, %f59, %f68, %f21589;
fma.rn.f32 %f71, %f60, %f68, %f21588;
        \end{minted}
        \captionof{listing}{Maschinencode des HIP-Kernels}
        \label{nvidia:nbody:ptxhip}
    \end{minipage}
\end{figure}

Die Abbildung~\ref{nvidia:nbody:comparisonsycl} zeigt die Ergebnisse des
Vergleichs zwischen CUDA (mit \texttt{Q\_rsqrt}) und SYCL. Beide
Implementierungen erreichen ähnliche Ergebnisse, wobei CUDA zumeist etwas
schneller arbeitet. Die Quelltexte~\ref{nvidia:nbody:ptxnvcc-qrsqrt} (CUDA) und
\ref{nvidia:nbody:ptxsycl} (SYCL) zeigen außerdem merkliche Unterschiede
zwischen den generierten Maschinen-Codes, was möglicherweise CUDAs bessere
Ergebnisse erklärt.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Leistungsvergleich -- CUDA und SYCL},
            xlabel = {$n$},
            ylabel = {GFLOPS},
            xtick = data,
            xmode = log,
            log basis x = 2,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma,
            ybar,
            width = 0.75\textwidth,
            scale only axis,
            ymin = 0, ymax = 4000,
            extra y ticks = 3935,
            extra y tick labels = {\num{3935}},
            extra y tick style={grid=major, major grid style={solid,thick,draw=red}},
            scaled y ticks = false,
            ylabel near ticks,
            xlabel near ticks
        ]
            \addplot table [x = n, y = CUDAQ, col sep = semicolon]
                           {data/nbody-nvidia.csv};
            \addlegendentry{CUDA} 

            \addplot table [x = n, y = SYCL, col sep = semicolon]
                           {data/nbody-nvidia.csv};
            \addlegendentry{SYCL} 
        \end{axis}
    \end{tikzpicture}
    \caption{Leistungsvergleich zwischen CUDA (mit Q\_rsqrt) und SYCL}
    \label{nvidia:nbody:comparisonsycl}
\end{figure}

\begin{figure}
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{minted}[fontsize=\footnotesize]{text}
ld.shared.v4.f32 {%f50, %f51, %f52,
                  %f53}, [%r31];
sub.f32 %f58, %f50, %f35;
sub.f32 %f59, %f51, %f36;
sub.f32 %f60, %f52, %f37;
mov.f32 %f61, 0f358637BE;
fma.rn.f32 %f62, %f60, %f60, %f61;
fma.rn.f32 %f63, %f59, %f59, %f62;
fma.rn.f32 %f64, %f58, %f58, %f63;
mul.f32 %f65, %f64, %f64;
mul.f32 %f66, %f64, %f65;
mul.f32 %f67, %f66, 0f3F000000;
mov.b32 %r32, %f66;
shr.s32 %r33, %r32, 1;
mov.u32 %r34, 1597463007;
sub.s32 %r35, %r34, %r33;
mov.b32 %f68, %r35;
mul.f32 %f69, %f67, %f68;
mul.f32 %f70, %f68, %f69;
mov.f32 %f71, 0f3FC00000;
sub.f32 %f72, %f71, %f70;
mul.f32 %f73, %f68, %f72;
mul.f32 %f74, %f53, %f73;
fma.rn.f32 %f75, %f58, %f74, %f13403;
fma.rn.f32 %f76, %f59, %f74, %f13402;
fma.rn.f32 %f77, %f60, %f74, %f13401;
        \end{minted}
        \captionof{listing}{Maschinencode des CUDA-Kernels (mit Q\_rsqrt)}
        \label{nvidia:nbody:ptxnvcc-qrsqrt}
    \end{minipage}
    %
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{minted}[fontsize=\footnotesize]{text}
ld.shared.v4.f32 {%f33, %f34, %f35,
                  %f36}, [%rd25];
sub.rn.f32 %f37, %f33, %f1;
sub.rn.f32 %f38, %f34, %f2;
sub.rn.f32 %f39, %f35, %f3;
fma.rn.f32 %f40, %f39, %f39,
           0f358637BE;
fma.rn.f32 %f41, %f38, %f38, %f40;
fma.rn.f32 %f42, %f37, %f37, %f41;
mul.rn.f32 %f43, %f42, %f42;
mul.rn.f32 %f44, %f42, %f43;
mul.rn.f32 %f45, %f44, 0fBF000000;
mov.b32 %r13, %f44;
shr.s32 %r14, %r13, 1;
sub.s32 %r16, %r15, %r14;
mov.b32 %f46, %r16;
mul.rn.f32 %f47, %f45, %f46;
mul.rn.f32 %f48, %f47, %f46;
add.rn.f32 %f49, %f48, 0f3FC00000;
mul.rn.f32 %f50, %f49, %f46;
mul.rn.f32 %f51, %f36, %f50;
fma.rn.f32 %f76, %f39, %f51, %f76;
fma.rn.f32 %f75, %f38, %f51, %f75;
fma.rn.f32 %f74, %f37, %f51, %f74;
        \end{minted}
        \captionof{listing}{Maschinencode des SYCL-Kernels}
        \label{nvidia:nbody:ptxsycl}
    \end{minipage}
\end{figure}
